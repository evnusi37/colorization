
***********************************
Richard Zhang / 2016.08.06
***********************************
This subfolder contains code for training a colorization network from Colorful Image Colorization, by Richard Zhang, Phillip Isola, Alexei A. Efros, In ECCV 2016.
***********************************

From the root directory of this repo:

(1) Run './train/fetch_init_model.sh'. This will load model './init_v2.caffemodel' into the './models' directory. This model was obtained using the k-means initialization implemented in [https://github.com/philkr/magic_init].

(2) Run './train/fetch_caffe.sh'. This will load a modified Caffe into directory './caffe-colorization'. Instructions on building Caffe can be found here [https://github.com/BVLC/caffe].

* Note that this is the same as vanilla-Caffe, with a 'SoftmaxCrossEntropyLayer' layer added. You likely can add the layer to your current build of Caffe by adding the following files (found in the './resources' directory) and re-compiling:
	./src/caffe/layers/softmax_cross_entropy_loss_layer.cpp
	./src/caffe/layers/softmax_cross_entropy_loss_layer.cu
	./include/caffe/layers/softmax_cross_entropy_loss_layer.hpp
If you do this, link your build of Caffe as './caffe-colorization'

(3) Add the './resources/' directory (as an absolute path) to your system environment variable $PYTHONPATH.

(4) Modify path in './models/colorization_train_val_v2.prototxt' to locate where ImageNet LMDB files are on your machine. These should be BGR images, non-mean centered.

(5) Run './train_model.sh [GPU_ID]', where [GPU_ID] is the gpu you choose to specify. Notes about training:

(a) Training schedule is drops learning rate at 225k iterations, and completes around 450k iterations. Training is done on mirrored and randomly cropped 176x176 resolution images, with mini-batch size 40.

(b) Snapshots every 1000 iterations will be saved in './train/models/colornet_[ITERNUMBER].caffemodel' and './train/models/colornet_iter_[ITERNUMBER].snapshot'.

(c) If training is interupted, resume training by running './train/train_resume.sh ./train/models/colornet_iter_[ITERNUMBER].snapshot [GPU_ID]', where [ITERNUMBER] is the last snapshotted model.

(d) Check validation loss by running './val_model.sh ./train/models/colornet_iter_[ITERNUMBER].caffemodel [GPU_ID] 1000', where [ITERNUMBER] is the model you would like to validate. This runs the first 10k imagenet validation images at full 256x256 resolution through the model. Validation loss on 

(e) Check model outputs by running the IPython notebook demo. Replacing release model with your snapshotted model.

(f) To download reference trained model, run './models/fetch_release_models.sh'. This will load reference model 'colorization_release_v2.caffemodel' into the './models' directory. This model was trained with this protocol and used to generate results in the arXiv v2/ECCV 2016 camera ready paper.

For completeness, this will also load model 'colorization_release_v2_norebal.caffemodel', which is was trained without class rebalancing, and 'colorization_release_v0.caffemodel', which was used to generate the results in the arXiv v1 paper.
